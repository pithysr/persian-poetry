{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from textblob import TextBlob\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score,learning_curve\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnotNaN(num):\n",
    "    return num == num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(review):\n",
    "    return TextBlob(review).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Load dataset... in `myl`]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POETS_LIST = ['khayyam', 'saadi', 'hafez', 'moulavi', 'ferdousi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POETS_INDEX = {x:i for i,x in enumerate(POETS_LIST)}\n",
    "POETS_INDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(myl, columns=['poet', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = df[['poet_cat','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(review):\n",
    "    return TextBlob(review).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head().text.apply(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kim CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_train, all_data_test = train_test_split(all_data ,\n",
    "                                                   random_state = 666 , shuffle = True,\n",
    "                                                   test_size = .15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = max([len(data) for sentence, _ in data])\n",
    "print('sentence maxlen', max_sentence_len)\n",
    "\n",
    "vocab = []\n",
    "for d, _ in data:\n",
    "    for w in d:\n",
    "        if w not in vocab: vocab.append(w)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab)\n",
    "w2i = {w:i for i,w in enumerate(vocab)}\n",
    "i2w = {i:w for i,w in enumerate(vocab)}\n",
    "div_idx = (int)(len(data) * 0.8)\n",
    "random.shuffle(data)\n",
    "train_data = data[:div_idx]\n",
    "test_data = data[div_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, out_chs, filter_heights):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embd_size)\n",
    "        self.conv = nn.ModuleList([nn.Conv2d(1, out_chs, (fh, embd_size)) \n",
    "                                   for fh in filter_heights])\n",
    "        self.dropout = nn.Dropout(.5)\n",
    "        self.fc1 = nn.Linear(out_chs*len(filter_heights), 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "        x = x.unsqueeze(1) \n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.conv]\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1) \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        probs = F.sigmoid(x)\n",
    "        return probs\n",
    "\n",
    "\n",
    "def train(model, data, batch_size, n_epoch):\n",
    "    model.train() \n",
    "    if use_cuda:\n",
    "        model.cuda()\n",
    "    losses = []\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_loss = 0.0\n",
    "        random.shuffle(data)\n",
    "        for i in range(0, len(data)-batch_size, batch_size):\n",
    "            in_data, labels = [], []\n",
    "            for sentence, label in data[i: i+batch_size]:\n",
    "                index_vec = [w2i[w] for w in sentence]\n",
    "                pad_len = max(0, max_sentence_len - len(index_vec))\n",
    "                index_vec += [0] * pad_len\n",
    "                index_vec = index_vec[:max_sentence_len] \n",
    "                in_data.append(index_vec)\n",
    "                labels.append(float(label))\n",
    "            sent_var = Variable(torch.LongTensor(in_data))\n",
    "            if use_cuda: sent_var = sent_var.cuda()\n",
    "\n",
    "            target_var = Variable(torch.Tensor(labels).unsqueeze(1))\n",
    "            if use_cuda: target_var = target_var.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(sent_var)\n",
    "            loss = F.binary_cross_entropy(probs, target_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.data[0]\n",
    "        losses.append(epoch_loss)\n",
    "        \n",
    "    return model, losses\n",
    "\n",
    "def test(model, data, n_test, min_sentence_len):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    for sentence, label in data[:n_test]:\n",
    "        if len(sentence) < min_sentence_len:  # to0 short for CNN's filter\n",
    "            continue\n",
    "        index_vec = [w2i[w] for w in sentence]\n",
    "        sent_var = Variable(torch.LongTensor([index_vec]))\n",
    "        if use_cuda: sent_var = sent_var.cuda()\n",
    "        out = model(sent_var)\n",
    "        score = out.data[0][0]\n",
    "#         templist = pd.DataFrame(test_data)[1].uniques()\n",
    "#         pred = 1 if score > .5 else 0\n",
    "        pred = 1 if score > .5 else 0\n",
    "        if pred == label:\n",
    "            correct += 1\n",
    "        loss += math.pow((label-score), 2)\n",
    "#     print(score, type(out))\n",
    "    print('Test acc: {:.3f} ({:d}/{:d})'.format(correct/n_test, correct, n_test))\n",
    "    print('Test loss: {:.3f}'.format(loss/n_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ch = 20\n",
    "embd_size = 64\n",
    "batch_size = 8\n",
    "n_epoch = 10\n",
    "filter_variations = [[1]]\n",
    "\n",
    "for fil in filter_variations:\n",
    "    model = Net(vocab_size, embd_size, out_ch, fil)\n",
    "    model, losses = train(model, train_data, batch_size, n_epoch)\n",
    "    test(model, test_data, len(test_data), max(fil))\n",
    "\n",
    "# model = Net(vocab_size, embd_size, out_ch, [1])\n",
    "# model, losses = train(model, train_data, batch_size, n_epoch)\n",
    "# test(model, test_data, len(test_data), max(fil))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
